
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Value in main function False
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
Value just before calling alexnetmodes False
num_classes: 1000
total_num_examples: 12288
fake_data/input_producer/FIFOQueueV2
fake_data/input_producer_1/FIFOQueueV2
4 threads started for queue
2019-04-18 16:00:31.404612: step 0, loss = 9.95 (34.3 examples/sec; 3.727 sec/batch)
2019-04-18 16:00:35.025040: step 1, loss = 1.95 (41.3 examples/sec; 3.101 sec/batch)
2019-04-18 16:00:38.014443: step 2, loss = 1.95 (42.9 examples/sec; 2.986 sec/batch)
2019-04-18 16:00:41.051765: step 3, loss = 1.95 (42.2 examples/sec; 3.034 sec/batch)
2019-04-18 16:00:44.026185: step 4, loss = 1.96 (43.1 examples/sec; 2.971 sec/batch)
2019-04-18 16:00:47.284502: step 5, loss = 1.96 (39.3 examples/sec; 3.255 sec/batch)
2019-04-18 16:00:50.353903: step 6, loss = 1.96 (41.8 examples/sec; 3.066 sec/batch)
2019-04-18 16:00:53.719694: step 7, loss = 1.96 (38.1 examples/sec; 3.362 sec/batch)
2019-04-18 16:00:56.746090: step 8, loss = 1.96 (42.3 examples/sec; 3.023 sec/batch)
2019-04-18 16:00:59.818006: step 9, loss = 1.96 (41.7 examples/sec; 3.068 sec/batch)
2019-04-18 16:01:02.882144: step 10, loss = 1.96 (41.8 examples/sec; 3.060 sec/batch)
2019-04-18 16:01:05.965586: step 11, loss = 1.96 (41.5 examples/sec; 3.081 sec/batch)
2019-04-18 16:01:09.057314: step 12, loss = 1.96 (41.4 examples/sec; 3.088 sec/batch)
2019-04-18 16:01:12.139039: step 13, loss = 1.96 (41.6 examples/sec; 3.078 sec/batch)
2019-04-18 16:01:15.188645: step 14, loss = 1.96 (42.0 examples/sec; 3.046 sec/batch)
2019-04-18 16:01:18.170913: step 15, loss = 1.96 (43.0 examples/sec; 2.980 sec/batch)
2019-04-18 16:01:21.207145: step 16, loss = 1.96 (42.2 examples/sec; 3.033 sec/batch)
2019-04-18 16:01:24.390209: step 17, loss = 1.96 (40.3 examples/sec; 3.179 sec/batch)
2019-04-18 16:01:27.401058: step 18, loss = 1.96 (42.6 examples/sec; 3.007 sec/batch)
2019-04-18 16:01:30.898106: step 19, loss = 1.96 (36.6 examples/sec; 3.493 sec/batch)
2019-04-18 16:01:33.916701: step 20, loss = 1.96 (42.5 examples/sec; 3.015 sec/batch)
2019-04-18 16:01:36.904250: step 21, loss = 1.96 (42.9 examples/sec; 2.984 sec/batch)
2019-04-18 16:01:40.184475: step 22, loss = 1.96 (39.1 examples/sec; 3.277 sec/batch)
2019-04-18 16:01:43.217656: step 23, loss = 1.96 (42.3 examples/sec; 3.030 sec/batch)
2019-04-18 16:01:46.287917: step 24, loss = 1.96 (41.7 examples/sec; 3.067 sec/batch)
2019-04-18 16:01:49.605754: step 25, loss = 1.96 (38.6 examples/sec; 3.314 sec/batch)
2019-04-18 16:01:52.858756: step 26, loss = 1.96 (39.4 examples/sec; 3.249 sec/batch)
2019-04-18 16:01:55.931656: step 27, loss = 1.96 (41.7 examples/sec; 3.069 sec/batch)
2019-04-18 16:01:58.954259: step 28, loss = 1.96 (42.4 examples/sec; 3.019 sec/batch)
2019-04-18 16:02:02.335018: step 29, loss = 1.96 (37.9 examples/sec; 3.377 sec/batch)
2019-04-18 16:02:05.434714: step 30, loss = 1.96 (41.3 examples/sec; 3.096 sec/batch)
2019-04-18 16:02:08.445974: step 31, loss = 1.96 (42.6 examples/sec; 3.008 sec/batch)
2019-04-18 16:02:11.410947: step 32, loss = 1.96 (43.2 examples/sec; 2.962 sec/batch)
2019-04-18 16:02:14.507978: step 33, loss = 1.96 (41.4 examples/sec; 3.093 sec/batch)
2019-04-18 16:02:17.588365: step 34, loss = 1.96 (41.6 examples/sec; 3.077 sec/batch)
2019-04-18 16:02:20.740704: step 35, loss = 1.95 (40.7 examples/sec; 3.149 sec/batch)
2019-04-18 16:02:23.899537: step 36, loss = 1.95 (40.6 examples/sec; 3.155 sec/batch)
2019-04-18 16:02:27.047679: step 37, loss = 1.95 (40.7 examples/sec; 3.144 sec/batch)
2019-04-18 16:02:30.451033: step 38, loss = 1.95 (37.7 examples/sec; 3.400 sec/batch)
2019-04-18 16:02:33.426115: step 39, loss = 1.95 (43.1 examples/sec; 2.973 sec/batch)
2019-04-18 16:02:36.485273: step 40, loss = 1.95 (41.9 examples/sec; 3.055 sec/batch)
2019-04-18 16:02:39.444176: step 41, loss = 1.95 (43.3 examples/sec; 2.955 sec/batch)
2019-04-18 16:02:42.458736: step 42, loss = 1.95 (42.5 examples/sec; 3.011 sec/batch)
2019-04-18 16:02:45.494740: step 43, loss = 1.95 (42.2 examples/sec; 3.032 sec/batch)
2019-04-18 16:02:48.510508: step 44, loss = 1.95 (42.5 examples/sec; 3.012 sec/batch)
2019-04-18 16:02:51.498623: step 45, loss = 1.95 (42.9 examples/sec; 2.984 sec/batch)
2019-04-18 16:02:54.526460: step 46, loss = 1.95 (42.3 examples/sec; 3.024 sec/batch)
2019-04-18 16:02:57.505163: step 47, loss = 1.95 (43.0 examples/sec; 2.975 sec/batch)
2019-04-18 16:03:00.934307: step 48, loss = 1.95 (37.4 examples/sec; 3.425 sec/batch)
2019-04-18 16:03:04.089145: step 49, loss = 1.95 (40.6 examples/sec; 3.151 sec/batch)
2019-04-18 16:03:07.243586: step 50, loss = 1.95 (40.6 examples/sec; 3.151 sec/batch)
2019-04-18 16:03:10.364585: step 51, loss = 1.95 (41.1 examples/sec; 3.118 sec/batch)
2019-04-18 16:03:13.520692: step 52, loss = 1.95 (40.6 examples/sec; 3.152 sec/batch)
2019-04-18 16:03:16.511434: step 53, loss = 1.95 (42.8 examples/sec; 2.988 sec/batch)
2019-04-18 16:03:19.559470: step 54, loss = 1.95 (42.0 examples/sec; 3.046 sec/batch)
2019-04-18 16:03:22.485483: step 55, loss = 1.95 (43.8 examples/sec; 2.922 sec/batch)
2019-04-18 16:03:26.031457: step 56, loss = 1.95 (36.1 examples/sec; 3.542 sec/batch)
2019-04-18 16:03:29.339310: step 57, loss = 1.95 (38.7 examples/sec; 3.305 sec/batch)
2019-04-18 16:03:32.763510: step 58, loss = 1.95 (37.4 examples/sec; 3.421 sec/batch)
2019-04-18 16:03:35.869849: step 59, loss = 1.95 (41.3 examples/sec; 3.103 sec/batch)
2019-04-18 16:03:38.932048: step 60, loss = 1.95 (41.9 examples/sec; 3.059 sec/batch)
2019-04-18 16:03:41.949422: step 61, loss = 1.95 (42.5 examples/sec; 3.014 sec/batch)
2019-04-18 16:03:45.076729: step 62, loss = 1.95 (41.0 examples/sec; 3.124 sec/batch)
2019-04-18 16:03:48.104322: step 63, loss = 1.95 (42.3 examples/sec; 3.024 sec/batch)
2019-04-18 16:03:51.402608: step 64, loss = 1.95 (38.8 examples/sec; 3.296 sec/batch)
2019-04-18 16:03:54.526509: step 65, loss = 1.95 (41.0 examples/sec; 3.120 sec/batch)
2019-04-18 16:03:57.665868: step 66, loss = 1.95 (40.8 examples/sec; 3.136 sec/batch)
2019-04-18 16:04:00.623320: step 67, loss = 1.95 (43.3 examples/sec; 2.954 sec/batch)
2019-04-18 16:04:03.610291: step 68, loss = 1.95 (42.9 examples/sec; 2.983 sec/batch)
2019-04-18 16:04:06.708123: step 69, loss = 1.95 (41.4 examples/sec; 3.094 sec/batch)
2019-04-18 16:04:09.808429: step 70, loss = 1.95 (41.3 examples/sec; 3.097 sec/batch)
2019-04-18 16:04:13.262886: step 71, loss = 1.95 (37.1 examples/sec; 3.451 sec/batch)
2019-04-18 16:04:16.265485: step 72, loss = 1.95 (42.7 examples/sec; 2.999 sec/batch)
2019-04-18 16:04:19.391189: step 73, loss = 1.95 (41.0 examples/sec; 3.122 sec/batch)
2019-04-18 16:04:22.445724: step 74, loss = 1.95 (41.9 examples/sec; 3.052 sec/batch)
2019-04-18 16:04:25.506750: step 75, loss = 1.95 (41.9 examples/sec; 3.057 sec/batch)
2019-04-18 16:04:28.602522: step 76, loss = 1.95 (41.4 examples/sec; 3.092 sec/batch)
2019-04-18 16:04:31.670111: step 77, loss = 1.95 (41.8 examples/sec; 3.064 sec/batch)
2019-04-18 16:04:34.684659: step 78, loss = 1.95 (42.5 examples/sec; 3.011 sec/batch)
2019-04-18 16:04:37.714102: step 79, loss = 1.95 (42.3 examples/sec; 3.027 sec/batch)
2019-04-18 16:04:40.957260: step 80, loss = 1.95 (39.5 examples/sec; 3.239 sec/batch)
2019-04-18 16:04:44.031488: step 81, loss = 1.95 (41.7 examples/sec; 3.071 sec/batch)
2019-04-18 16:04:47.089709: step 82, loss = 1.95 (41.9 examples/sec; 3.054 sec/batch)
2019-04-18 16:04:50.139420: step 83, loss = 1.95 (42.0 examples/sec; 3.046 sec/batch)
2019-04-18 16:04:53.243937: step 84, loss = 1.95 (41.3 examples/sec; 3.101 sec/batch)
2019-04-18 16:04:56.236997: step 85, loss = 1.95 (42.8 examples/sec; 2.991 sec/batch)
2019-04-18 16:04:59.284249: step 86, loss = 1.95 (42.1 examples/sec; 3.044 sec/batch)
2019-04-18 16:05:02.356410: step 87, loss = 1.95 (41.7 examples/sec; 3.068 sec/batch)
2019-04-18 16:05:05.591317: step 88, loss = 1.95 (39.6 examples/sec; 3.231 sec/batch)
2019-04-18 16:05:08.609442: step 89, loss = 1.95 (42.5 examples/sec; 3.014 sec/batch)
2019-04-18 16:05:11.643409: step 90, loss = 1.95 (42.2 examples/sec; 3.030 sec/batch)
2019-04-18 16:05:14.622047: step 91, loss = 1.95 (43.0 examples/sec; 2.975 sec/batch)
2019-04-18 16:05:17.987918: step 92, loss = 1.95 (38.1 examples/sec; 3.363 sec/batch)
2019-04-18 16:05:20.998128: step 93, loss = 1.95 (42.6 examples/sec; 3.006 sec/batch)
2019-04-18 16:05:24.131582: step 94, loss = 1.95 (40.9 examples/sec; 3.131 sec/batch)
2019-04-18 16:05:27.288085: step 95, loss = 1.95 (40.6 examples/sec; 3.153 sec/batch)
Average 41.2 examples/sec
